# 自然语言处理高级专题 大作业

> 姓名：王晨朔
> 学院：前沿交叉学科研究院
> 学号：2001213250

## 任务介绍

词性标注（POS Tagging）是自然语言处理的基本任务，也是各类自然语言处理应用的基础。给定一个句子 $\{x_i\}_{i=1}^n$，其中 $x_i$ 表示第 $i$ 个字，要求将这个句子进行分词，并得到词性标签序列 $\{y_i\}_{i=1}^m$，其中 $m$ 是句子中词的数量。

词性标注是一个经典任务，研究者们从基于规则、基于特征的方法，到近几年关注基于深度网络的方法。18 年来，基于 transformers 的预训练模型在自然语言处理领域受到极大关注，也被用于词性标注领域。

这次实验中，我们实现了基于 BERT + Softmax 序列标注的方法，并对结果进行了分析。

## 方法

我们采用了 BERT 模型。BERT 是由 12 层 transformers encoder 和 decoder 组成的。通过 MLM，NSP 的预训练任务，得到预训练的模型。为了兼顾简体与繁体，我们使用哈工大预训练的 chinese-bert-wwm-ext 参数，并在提供的语料上 finetune。使用 BERT 在下游任务上 finetune 如下图 d 所示：

![](https://pic2.zhimg.com/80/v2-b054e303cdafa0ce41ad761d5d0314e1_1440w.jpg)

由于中文 BERT 是以字为单位输入，因此，我们首先将训练数据拆分成以字为单位的句子。具体地，对于一个词及其标签 $X/y$，我们将其拆成 $X_1/B-y X_2/I-y X_3/I-y ...$，将标签类别数扩大了两倍。预测得到的结果再根据相同的规则进行组合。

后来在群里听说，提交结果需要使用给定的分词结果，因此我们又取消使用 $BI$ 的标签，即对于一个词及其标签 $X/y$，我们将其拆成 $X_1/y X_2/y X_3/y ...$，预测结果中，词的标签由第一个字的标签决定。

## 实验

我们在繁体 / 简体数据集上，将数据按照 7:3 划分，分别进行了训练和评测，结果如下表所示：

| model | data 1 | data 2 |
| ----- | ------ | ------ |
| BERT + softmax | 0.9466 | 0.9580 |

以下给出一些错误例子，并尝试给出改进的方法：

预测错误多是因为标签的粒度问题，例如：

> 预测：接受/v  这次/r  调查/vn  的/ud  德国/ns  企业/n  共/d  １９９/m  家/qe
> 真实：接受/v  这次/r  调查/v  的/ud  德国/ns  企业/n  共/d  １９９/m  家/qe

这里模型错把“调查/v”预测成“调查/vn”。我们猜测，vn 应该是属于 v 的一个标签，粒度更细。因此，未来的工作中，我们将尝试使用分层的标签系统，输入每个 token 在不同粒度下的标签作为训练，在标签树中找到最优的标签。

> 预测：港股/Nc  昨日/Nt  低開/Vi  逾/Vt  八百/Mc  點/Qc  ，/Sy  投資者/Nc  憂慮/Vt  外圍/Nc  股市/Nc  調整/Vn  尚未/Dc  完結/Vi
> 真实：港股/Nc  昨日/Nt  低開/Vi  逾/Vt  八百/Mc  點/Qc  ，/Sy  投資者/Nc  憂慮/Vt  外圍/Nc  股市/Nc  調整/Vt  尚未/Dc  完結/Vi

在这个例子中，模型错把“調整/Vt”预测为“調整/Vn”。我们猜测，Vt 与 Vn 应该是属于同一个大类的两个小类。我们认为，现在这种多分类的方法是不太合理的，现在的多分类方法，相当于在高维空间中划分出多个互不相容的子空间，并一致同仁地让每个标签所属的例子尽量归到标签子空间中。

但实际上，标签是具有层次结构的，例如，标签 V 就包含了 Vt 和 Vn。我们可以借鉴对比学习的方法，采取 triplet loss，对于 Vt 标签的样例，我们应当以 Vt 及其所有父类作为正例，其余类别作为负例；对 Vn 标签的样例也是同理，应该令 V 和 Vn 标签的样例作为正类，Vt 以及其它标签的样例作为父类。这样就能自然地建模标签的层次关系。

## 总结

这门课程的大作业设置真的非常有意义！文本分类、实体抽取、关系抽取、序列标注，这些都是自然语言处理基础任务，为我们深入的研究打下坚实基础。在这里感谢老师和助教的辛苦付出！祝这门课越办越好！